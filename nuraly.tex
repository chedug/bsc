%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                            %
%    Dyussenov Nuraly BSc Thesis Work        %
%                                            %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt,a4paper,oneside]{book} % twoside,openany

% Packages
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{cancel}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\graphicspath{{./figures/}}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage{url}
\usepackage{hyperref}
\usepackage{refcheck}
\usepackage{tikz}

% Theoremlike environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}

% Ususal abbreviations
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}

% Probability theory
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\M}{\mathcal{M}}

\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\law}[1]{\text{Law}(#1)}

\newcommand{\Pas}{\text{a.s.}}
\newcommand{\ind}{\mathds{1}}

% Analysis
\newcommand{\eps}{\varepsilon}
\newcommand{\la}{\lambda}
\newcommand{\ga}{\gamma}
\newcommand{\ka}{\kappa}
\newcommand{\dtv}{d_{\text{TV}}}

% Integration
\newcommand{\dint}{\mathrm{d}} 

\newcommand{\lfrf}[1]{\lfloor #1\rfloor}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55% Page layout
\usepackage{indentfirst}
%\usepackage{fullpage}
\usepackage[a4paper]{geometry}
\geometry{tmargin=3cm,lmargin=3.5cm,rmargin=2cm}
% manual page formatting
%\setlength{\headsep}{25pt}
\hyphenation{}

% headers and footers
\usepackage{fancyhdr}
\usepackage{mathptmx}

\newcommand\HRule{\rule{\textwidth}{1pt}}

\usepackage{varwidth}

%\usepackage{booktabs}
\usepackage{multirow,array}
\usepackage{siunitx}

% Quotations
\usepackage{csquotes}

\makeatletter
\renewcommand{\@chapapp}{}% Not necessary...
\newenvironment{chapquote}[2][2em]
{\setlength{\@tempdima}{#1}%
	\def\chapquote@author{#2}%
	\parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
	\itshape}
{\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother


\usepackage{titlesec, blindtext, color}
\definecolor{gray75}{gray}{0.75}
\newcommand{\hsp}{\hspace{20pt}}


\newcommand{\stdwidth}{0.61\linewidth}

% Space above chapter titles
\usepackage{titlesec}

\renewcommand{\baselinestretch}{1.5}



\begin{document}

\begin{titlepage}
	
	%\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
	
	\center % Center everything on the page
	
	%----------------------------------------------------------------------------------------
	%	HEADING SECTIONS
	%----------------------------------------------------------------------------------------
	
	\textsc{\LARGE Budapesti University of Technology and Economics}\\[1.5cm] % Name of your university/college
	\textsc{\Large Institute of Mathematics}\\[0.5cm] % Major heading such as course name
	\textsc{\large Faculty of Mathematics}\\[0.5cm] % Minor heading such as course title
	
	%----------------------------------------------------------------------------------------
	%	TITLE SECTION
	%----------------------------------------------------------------------------------------
	
	\HRule \\[0.4cm]
	{ \Large \bfseries Linear Regression through Origin
		 }\\[0.4cm]
	\HRule \\[1.5cm]
	
	%----------------------------------------------------------------------------------------
	%	AUTHOR SECTION
	%----------------------------------------------------------------------------------------
	
	\begin{tabular}{L{6cm} R{8cm}}
	\emph{Author:}   & \emph{Supervisor:} \\
	Dyussenov Nuraly & Dr. Jozsef Mala   \\
	                 & Associate Professor, BME Fac. of Nat. Sci. 
	\end{tabular}\\[1.3cm]

	
	\vfill
	{\Large Budapest, \today}\\[1.2cm] % Date, change the \today to a set date if you want to be precise
		
	\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=0.5\linewidth]{bme_logo_nagy.eps}
	 % Include a department/university logo - this will require the graphicx package
	
	%\vfill % Fill the rest of the page with whitespace
	
\end{titlepage}

	%\newpage\null\thispagestyle{empty}\newpage

	\frontmatter
	%\chapter*{\centering Kivonat}
	
	\tableofcontents
	\listoftables
	\listoffigures
		
	\mainmatter
	
	\fancypagestyle{plain}{%

		\fancyhf{}
		\fancyhead[L]{\rule[-2ex]{0pt}{2ex}\small \leftmark} 
		\fancyhead[R]{} 
		\fancyfoot[L]{}
		\fancyfoot[C]{-- \thepage\ --}
		\fancyfoot[R]{} 
		\renewcommand{\headrulewidth}{1.5pt}
		\renewcommand{\footrulewidth}{1pt}}
	\pagestyle{plain}
	
	\titleformat{\chapter}[display]{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
	\titlespacing*{\chapter}{10pt}{20pt}{40pt}
	 
	\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter.\hsp}{0pt}{\Huge\bfseries} 
	  
	\chapter{Introduction} 
	
	\begin{chapquote}{Some funny guy} % Quotation (optional)
	''Let epsilon be negative.''
	\end{chapquote}


	\section{Introduction}

% Start your introduction with a general statement about linear regression through the origin
Linear regression through the origin is a specialized form of linear regression where the regression line passes through the origin point (\(0, 0\)). In this variant, the relationship between the dependent variable \(y\) and the independent variable \(x\) is represented by the equation:
\[ y = \beta x + \varepsilon \]
where \(\beta\) is the slope of the line, and \(\varepsilon\) represents the error term.

% Explain the importance and relevance of linear regression through the origin
Linear regression through the origin is particularly useful in situations where it is essential to force the regression line to go through the origin, indicating a direct proportional relationship between the variables without any offset. This constraint simplifies the model and is valuable in various scientific and engineering applications.

% State the objective of your thesis
The objective of this thesis is to delve into the nuances of linear regression through the origin. We aim to explore the mathematical foundations of this regression variant, discuss methods for estimating the parameter \(\beta\), and analyze its implications on model interpretation and prediction. By conducting a comprehensive study and practical analysis, we intend to provide valuable insights into the application of linear regression through the origin in real-world contexts.

% Provide a brief overview of the structure of the thesis
In the following chapters, we will focus on the specific aspects of linear regression through the origin. We will derive the regression equation, explore methods for estimating the slope \(\beta\), and investigate techniques for evaluating the model's goodness of fit. Practical examples and numerical illustrations will be used to reinforce the concepts discussed. Additionally, we will compare the results obtained from regression through the origin with those of traditional linear regression models. Finally, we will draw conclusions based on our findings and propose directions for further research in this domain.

% End the introduction section
This introductory chapter sets the stage for the subsequent discussions and analyses. By examining linear regression through the origin with mathematical rigor, we aim to enhance the understanding of this specialized regression technique and its practical applications in scenarios demanding an origin-centered approach.

% You can add more specific details or modify the content according to your research focus and preferences.

	
	

	\chapter{Theoretical background} % ~10 pages (by Dec 31)
	% Every chapter should start with a short summary


	\section{Statistics Basics}
	% Give definitions of mean, variance, statistic, estimators, sufficiency, bias, MSE, and etc.

\textbf{Definition (Data)} Let $(x_1, \ldots, x_n)$, where $x_i \in S$ for $i = 1, \ldots, n$. The set $S$ is typically $\mathbb{R}$, $\mathbb{R}^d$, or it can be any abstract set. However, for our purposes, $S$ (the sample space) will usually be $\mathbb{R}$.

\textbf{Definition (Sample)} In statistics, our data are often modeled by a vector $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ of i.i.d. (independent, identically distributed) random variables, called the sample (of which size is $n$), where the random variables $X_i$ take values in $\mathbb{Z}$ or $\mathbb{R}$. The common distribution of the $X_i$ is called the parent distribution, and we say that the sample is from that parent distribution.

\textbf{Definition (Model)} A statistical model is a family $\{P_{\theta} \,|\, \theta \in \Theta\}$ of distributions on the sample space. When $\Theta \subset \mathbb{R}^d$, we say that we have a parametric model, and we call $\Theta$ the parameter set (space).

\textbf{Definition (p-th Quantile of Data)} If $p \in (0, 1)$, then a $p$-th quantile (or a $p$-th percentile) of the data $(x_1, \ldots, x_n)$ is a $p$-th quantile of the corresponding empirical distribution function $\hat{F}_n$.

\textbf{Definition (Sample mean)} Let $(X_1, \ldots, X_n)$ be a sample. Then the random variable
\[ \bar{X} = X = \frac{1}{n} \sum_{i=1}^{n} X_i \]
is called the sample mean.

\textbf{Definition (Estimator)} An estimator is a statistic (a function of the sample data) used to estimate an unknown parameter in a statistical model. An estimator for the parameter $\theta$, denoted as $\hat{\theta}$, is any measurable function of the random variables $X_1, X_2, \ldots, X_n$.

\textbf{Definition (Unbiased Estimator)} If $\hat{\theta}$ is an estimator of $\theta$, then we can define the quantity \textit{Bias}($\hat{\theta}$) = $\mathbb{E}_{\theta}[\hat{\theta}] - \theta$. The estimator $\hat{\theta}$ is called unbiased if its bias is 0.

\textbf{Definition (MSE of an Estimator)} Let us have the model $\{P_{\theta} \,|\, \theta \in \Theta\}$ and let us have the sample $(X_1, \ldots, X_n)$ from it. The mean square error (or the quadratic risk) of an estimator $\hat{\theta} = \hat{\theta}(X_1, \ldots, X_n)$ for the parameter $\theta$ is defined by
\[ \text{MSE}_{\theta}(\hat{\theta}) = \mathbb{E}_{\theta}((\hat{\theta} - \theta)^2) \]
when $\theta$ is the true parameter.

\textbf{Steiner's identity:} $\mathbb{E}((X - a)^2) = \text{Var}(X) + (a - \mathbb{E}(X))^2$

\textbf{Interpretation in the context of mean square error (MSE):}

\[ \text{MSE}_{\theta}(\hat{\theta}) = \text{Var}_{\theta}(\hat{\theta}) + (\text{Bias}_{\theta}(\hat{\theta}))^2 \]

\textbf{Definition (Sufficiency)} Let the model be $\{P_{\theta} \,|\, \theta \in \Theta\}$ and $\mathbf{X} = (X_1, \ldots, X_n)$ be a sample from it. The statistic $T$ is called \textit{sufficient} for the parameter $\theta$ (or, for the model $\{P_{\theta} \,|\, \theta \in \Theta\}$) if the conditional distribution $P_{\theta}(\mathbf{X} \in \cdot \,|\, T = t)$ does not depend on $\theta$.

\textbf{Theorem (Neyman-Fisher Factorization Theorem)} If the model is $\{p(x|\theta) \,|\, \theta \in \Theta\}$ where $p(x|\theta)$ is a probability mass/density function and $\mathbf{X} = (X_1, \ldots, X_n)$ is a sample from it, then the statistic $T$ is \textit{sufficient} for the parameter $\theta$ if and only if we can find nonnegative functions $g$ and $h$ such that
\[ p_{\mathbf{X}}(x | \theta) = g(T(x), \theta)h(x). \]

\textbf{Definition (Likelihood)} Let $\{p(x, \theta), \theta \in \Theta\}$ be a model. If the observed value of $X$ is $x$, we say that $p(x | \theta)$ is the \textit{likelihood} of $\theta$: $L(\theta) = p(x | \theta)$. Thus, we are considering the mass/density as a function of $\theta$, for a fixed $x$. If $x = (x_1, \ldots, x_n)$ is a realization of the sample $\mathbf{X} = (X_1, \ldots, X_n)$, then $p(x | \theta)$ is the product of the marginals,
\[ L(\theta) = p(x | \theta) = \prod_{i=1}^{n} p(x_i | \theta). \]

\textbf{Theorem (Rao-Blackwell)} Let $\{P_{\theta} \,|\, \theta \in \Theta\}$ be a model and $(X_1, \ldots, X_n)$ be a sample. Let $\hat{\theta}$ be an estimator of $\theta$ with $\text{Var}_{\theta}(\hat{\theta})$ finite for each $\theta$. If $T$ is a sufficient statistic for $\theta$, then $\theta^* = \mathbb{E}_{\theta}(\hat{\theta} | T)$ is a statistic, and we have for all $\theta$ that
\[ \text{MSE}_{\theta}(\theta^*) \leq \text{MSE}_{\theta}(\hat{\theta}) \quad (1) \]
and the inequality is strict unless $\hat{\theta}$ is a function of $T$ with probability $1$.





	\clearpage
	
	\section{Simple Linear Regression}
	% Definitions of SLR

Consider the model function:
\[
y = \beta_0 + \beta_1 x,
\]
which describes a line with slope $\beta_1$ and y-intercept $\beta_0$. In general, such a relationship may not hold exactly for the largely unobserved population of values of the independent and dependent variables; we call the unobserved deviations from the above equation the errors. Suppose we observe $n$ data pairs and call them $\{(x_i, y_i), i = 1, \ldots, n\}$. We can describe the underlying relationship between $y_i$ and $x_i$ involving this error term $\varepsilon_i$ by:
\[
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i.
\]
This relationship between the true (but unobserved) underlying parameters $\beta_0$ and $\beta_1$ and the data points is called a linear regression model.

The goal is to find estimated values $\hat{\beta_0}$ and $\hat{\beta_1}$ for the parameters $\beta_0$ and $\beta_1$ which would provide the "best" fit in some sense for the data points. As mentioned in the introduction, in this article the "best" fit will be understood as in the least-squares approach: a line that minimizes the sum of squared residuals (see also Errors and residuals) $\hat{\varepsilon}_i$ (differences between actual and predicted values of the dependent variable $y$), each of which is given by, for any candidate parameter values $\beta_0$ and $\beta_1$:
\[
\hat{\varepsilon}_i = y_i - \beta_0 - \beta_1 x_i.
\]
In other words, $\hat{\beta_0}$ and $\hat{\beta_1}$ solve the following minimization problem:
\[
\text{Find } \min_{\beta_0, \beta_1} Q(\beta_0, \beta_1), \quad \text{for } Q(\beta_0, \beta_1) = \sum_{i=1}^{n} \hat{\varepsilon}_i^2 = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2.
\]

We assume that the response variable is normally distributed as follows: 

\[ Y_i \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2), \quad i = 1, \ldots, n \]
such that the \(Y_i\) are independent and \(\beta_0\), \(\beta_1\), and \(\sigma\) are unknown parameters.

If $x = (x_1, \ldots, x_n)$, $y = (y_1, \ldots, y_n)$, and we let $S_{xy} = \sum_{i=1}^{n} x_i y_i - n\bar{x}\bar{y}$, then it is easy to see that
\[ S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}). \]


Likelihood function
\[
L(y_1, \ldots, y_n | \beta_0, \beta_1, \sigma) = \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 \right)
\]

Log-likelihood function
\[
l(y_1, \ldots, y_n | \beta_0, \beta_1, \sigma) = c - n \log \sigma - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
\]


% Equations
\begin{align*}
\frac{\partial l}{\partial \beta_0} &= \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i)) = 0 \\
\frac{\partial l}{\partial \beta_1} &= \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))x_i = 0  \\
\frac{\partial l}{\partial \sigma} &= \frac{n}{\sigma} - \frac{1}{\sigma^3} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 = 0 
\end{align*}

% Text
Let the solutions of the above equations be denoted as $\hat{\beta}_0$, $\hat{\beta}_1$, $\hat{\sigma}^2$ for $\beta_0$, $\beta_1$, $\sigma^2$. If $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$, then...

% Equations
\begin{align*}
\hat{\beta}_0  &= \bar{y} - \hat{\beta}_1 \bar{x}; \\
\hat{\beta}_1 &= \frac{S_{xy}}{S_{xx}}; \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} SSE.
\end{align*}


% Proposition and Proof
\textbf{Proposition}
\[
\sum_{i=1}^{n} (y_i - \bar{y})^2 = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

% Proof (continued)
\textit{Proof .} The vector $\textbf{y} - \textbf{$\hat{y}$}$ is perpendicular to $\textbf{$\hat{y}$} - \textbf{1 $\bar{y}$}$, thus the proposition is true by the Pythagorean theorem.


% Proposition
\textbf{Proposition 1.4.} The estimators $\hat{\beta}_0$, $\hat{\beta}_1$, $\frac{SSE}{n-2}$ are unbiased estimators of $\beta_0$, $\beta_1$, $\sigma^2$ respectively.


% Proof that the estimators are unbiased
\textbf{Proof:}

1. \textbf{Unbiasedness of $\hat{\beta}_1$:}
\[
\mathbb{E} [ \hat{\beta _1} ] = \mathbb{E} [\frac{S_{xy}}{S_{xx}}] = \mathbb{E} [\frac{\Sigma (x_i - \bar{x})(y_i-\bar{y})}{\Sigma (x_i-\bar{x})^2}]=
\]

\[
	= \mathbb{E} [\frac{\Sigma (x_i - \bar{x})y_i}{\Sigma (x_i-\bar{x})^2}]=
	\frac{\Sigma (x_i - \bar{x})\mathbb{E} [y_i]}{\Sigma (x_i-\bar{x})^2}=
\]

\[
	= \frac{\Sigma (x_i - \bar{x})(\beta_0+\beta_1x_i)}{\Sigma (x_i-\bar{x})^2}=	
	\frac{\Sigma (x_i \beta_0 - \bar{x}\beta_0+\beta_1x_i^2-\beta_1x_i\bar{x})}{\Sigma x_i^2-n\bar{x}^2}=
\]

\[
	= \frac{\cancel{n\bar{x}\beta_0}-\cancel{n\bar{x}\beta_0}+\Sigma\beta_1x_i^2-n\beta_1\bar{x}^2}{\Sigma x_i^2-n\bar{x}^2}= \frac{(\Sigma x_i^2-n\bar{x}^2)\beta_1}{\Sigma x_i^2-n\bar{x}^2} = \beta_1
\]


2. \textbf{Unbiasedness of $\hat{\beta}_0$:}
\[
\mathbb{E}(\hat{\beta}_0) = \mathbb{E}(\bar{y} - \hat{\beta}_1 \bar{x}) = \bar{y} - \bar{x} \mathbb{E}(\hat{\beta}_1) = \frac{1}{n}\mathbb{E}[\Sigma y_i]-\beta_1 \bar{x} =
\] 

\[
	= \frac{1}{n}\mathbb{E}[\Sigma (\beta_0+\beta_1x_i)]-\beta_1 \bar{x} 
	= \frac{1}{n}n\beta_0+\frac{1}{n}n\beta_1\bar{x}-\bar{x}\beta_1=\beta_0	
\]


3. \textbf{Unbiasedness of $\frac{SSE}{n-2}$ as an estimator of $\sigma^2$:}
\[
\mathbb{E}\left(\frac{SSE}{n-2}\right) = \frac{1}{n-2} (n - 2) \sigma^2 = \sigma^2
\]




	\clearpage

	\section{Simple Linear Regression with no intercept term}

In simple linear 




	
	\clearpage

	\section{Comparative Analysis}	
	% If applicable, compare Linear Regression through Origin with other regression techniques. Discuss why this particular method was chosen over others, highlighting its advantages and limitations in comparison to traditional linear regression models.
	
	\clearpage

	
	\chapter{Applications to Linear Regression through Origin}


	\clearpage

	\section{Something to add 1} 
	% UNICORNS, Temporary Outlier Factor
	% Outlier types (point, trend and contextual outliers)
	
	\section{Something to add 1} 
	% Sugihara's algorithm (CCM) and the new entropy-based methods

	\chapter{Theoretical results} % (By Feb 26)
	
	\section{A theoretical resilt}
	
	\section{Towards some advanced topic}
	% Why is it desirable to establish such result? Do a short literature review. State a conjecture on the potential form of Taken's embedding theorem. Try to prove it (or at least test it) for autoregressive processes.

	\chapter{Programming simulations} % (By March 31)
	% Demonstrate the applicability of these algorithms on interesting data sets
	% Implementation details, how to find optimal time lag, and embedding dimension


		
	\chapter{Summary and closing words}
	% 1 page
	



	% All in all 26-30 pages
	
	% IRODALOMJEGYZÃ‰K
	\bibliography{nuraly}
	\bibliographystyle{plain}
	
	\appendix
	
	\chapter{Program Codes}
	
\end{document}
